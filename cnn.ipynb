{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from https://colab.research.google.com/drive/1ow8ePMoYAhBKyKOuH1JHmxPYFvOteBQH?usp=sharing#scrollTo=cQRhLUHFAmU6\n",
    "# Code taken from https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-from-scratch-for-mnist-handwritten-digit-classification/\n",
    "\n",
    "# Import libraries\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.legend_handler import HandlerLineCollection, HandlerTuple\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.metrics import Accuracy, Precision, Recall, F1Score\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from numpy import mean, std\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data = pd.read_csv('digit-recognizer/train.csv')\n",
    "    x = data.loc[:, data.columns != \"label\"]\n",
    "    y = data['label']\n",
    "    # Split train and test data\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
    "    # Reshape data\n",
    "    xtrain = xtrain.values.reshape((xtrain.shape[0], 28, 28, 1))\n",
    "    xtest = xtest.values.reshape((xtest.shape[0], 28, 28, 1))\n",
    "    ytrain = np.array(ytrain)\n",
    "    ytest = np.array(ytest)\n",
    "    # Normalize x\n",
    "    xtrain_norm = xtrain.astype('float32') / 255.0\n",
    "    xtest_norm = xtest.astype('float32') / 255.0\n",
    "    # Make labels categorical\n",
    "    ytrain = to_categorical(ytrain)\n",
    "    ytest = to_categorical(ytest)\n",
    "    return xtrain_norm, ytrain, xtest_norm, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN model\n",
    "def define_model():\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, (3, 3), padding = \"same\", activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(Dense(10, activation='softmax'))\n",
    "\topt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy', Precision, Recall, F1Score])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a model using k-fold cross-validation\n",
    "def evaluate_model(dataX, dataY, n_folds=5):\n",
    "\tscores, histories = list(), list()\n",
    "\t# Prepare cross validation\n",
    "\tkfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "\t# Enumerate splits\n",
    "\tfor train_ix, test_ix in kfold.split(dataX):\n",
    "\t\t# Define model\n",
    "\t\tmodel = define_model()\n",
    "\t\t# Select rows for train and test\n",
    "\t\ttrainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]\n",
    "\t\t# Fit model\n",
    "\t\thistory = model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=0)\n",
    "\t\t# Evaluate model\n",
    "\t\t_, acc, prec, rec, f1 = model.evaluate(testX, testY, verbose=0)\n",
    "\t\tprec = mean(prec)\n",
    "\t\tprint('Accuracy > %.3f' % (acc * 100.0))\n",
    "\t\t# print('Precision > %.3f' % (prec * 100.0))\n",
    "\t\t# print('Recall > %.3f' % (rec * 100.0))\n",
    "\t\t# print('F1 Score > %.3f' % (f1 * 100.0))\n",
    "\t\t# Store scores\n",
    "\t\tscores.append([acc, prec, rec, f1])\n",
    "\t\thistories.append(history)\n",
    "\treturn scores, histories, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot diagnostic learning curves\n",
    "def summarize_diagnostics(histories):\n",
    "\tfor i in range(len(histories)):\n",
    "\t\t# Plot loss\n",
    "\t\tplt.title('Cross Entropy Loss')\n",
    "\t\tplt.plot(histories[i].history['loss'], color='blue', label='train')\n",
    "\t\tplt.plot(histories[i].history['val_loss'], color='orange', label='test')\n",
    "\t\tplt.xticks([0, 2, 4, 6, 8], [1, 2, 3, 4, 5])\n",
    "\t\tplt.xlabel(\"Fold K\")\n",
    "\t\tplt.ylabel(\"Loss\")\n",
    "\tplt.legend(loc=\"upper right\", handler_map={tuple: HandlerTuple(ndivide=None)})\n",
    "\tplt.show()\n",
    "\tfor i in range(len(histories)):\n",
    "\t\t# Plot accuracy\n",
    "\t\tplt.title('Classification Accuracy')\n",
    "\t\tplt.plot(histories[i].history['accuracy'], color='blue', label='train')\n",
    "\t\tplt.xticks([0, 2, 4, 6, 8], [1, 2, 3, 4, 5])\n",
    "\t\tplt.plot(histories[i].history['val_accuracy'], color='orange', label='test')\n",
    "\t\tplt.xlabel(\"Fold K\")\n",
    "\t\tplt.ylabel(\"Accuracy\")\n",
    "\tplt.legend(loc=\"lower right\", handler_map={tuple: HandlerTuple(ndivide=None)})\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize model performance\n",
    "def summarize_performance(scores):\n",
    "\tprint('Accuracy: mean=%.3f std=%.3f, n=%d' % (mean(scores[:, 0])*100, std(scores[:, 0])*100, len(scores[:, 0])))\n",
    "\tprint('Precision: mean=%.3f std=%.3f, n=%d' % (mean(scores[:, 1])*100, std(scores[:, 1])*100, len(scores[:, 1])))\n",
    "\tprint('Recall: mean=%.3f std=%.3f, n=%d' % (mean(scores[:, 2])*100, std(scores[:, 2])*100, len(scores[:, 2])))\n",
    "\tprint('F1 Score: mean=%.3f std=%.3f, n=%d' % (mean(scores[:, 3])*100, std(scores[:, 3])*100, len(scores[:, 3])))\n",
    "\tplt.boxplot(scores)\n",
    "\tplt.xticks([1, 2, 3, 4], ['Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\tplt.xlabel(\"Metric\")\n",
    "\tplt.ylabel(\"Performance\")\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class estimator:\n",
    "    _estimator_type = ''\n",
    "    classes_=[]\n",
    "    def __init__(self, model, classes):\n",
    "        self.model = model\n",
    "        self._estimator_type = 'classifier'\n",
    "        self.classes_ = classes\n",
    "    def predict(self, X):\n",
    "        y_prob= self.model.predict(X)\n",
    "        y_pred = y_prob.argmax(axis=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# load dataset\n",
    "\ttrainX, trainY, testX, testY = load_data()\n",
    "\t# evaluate model\n",
    "\tscores, histories, model = evaluate_model(trainX, trainY)\n",
    "\t# learning curves\n",
    "\tsummarize_diagnostics(histories)\n",
    "\t# summarize estimated performance\n",
    "\tsummarize_performance(np.array(scores))\n",
    "\t# Get results from test set\n",
    "\tclassifier = estimator(model, list(range(0, 10)))\n",
    "\tpredictedY = classifier.predict(testX)\n",
    "\ttestY = np.array(pd.from_dummies(pd.DataFrame(testY)))\n",
    "\tprint(f\"Accuracy: {round(accuracy_score(testY, predictedY), 4) * 100}%\")\n",
    "\tprint(f\"Precision: {round(precision_score(testY, predictedY, average = 'macro'), 4) * 100}%\")\n",
    "\tprint(f\"Recall: {round(recall_score(testY, predictedY, average = 'macro'), 4) * 100}%\")\n",
    "\tprint(f\"F1 Score: {round(f1_score(testY, predictedY, average = 'macro'), 4) * 100}%\")\n",
    "\tConfusionMatrixDisplay.from_estimator(estimator=classifier, X = testX, y = testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model()\n",
    "plot_model(model, \"cnn_model.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
